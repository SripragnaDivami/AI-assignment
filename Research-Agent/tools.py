# tools.py
import asyncio
from typing import Dict, Any, List
import aiohttp
import math
from tenacity import retry, stop_after_attempt, wait_exponential
import logging

logger = logging.getLogger(__name__)

# --- Simple web fetch tool (minimal) ---
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=8))
async def web_fetch(url: str, timeout: int = 8) -> Dict[str, Any]:
    """
    Minimal web fetcher: returns status and text snippet.
    Retries on transient failure.
    """
    logger.debug("web_fetch called with url=%s", url)
    async with aiohttp.ClientSession() as session:
        async with session.get(url, timeout=timeout) as resp:
            text = await resp.text()
            snippet = (text[:400] + '...') if len(text) > 400 else text
            return {"url": url, "status": resp.status, "snippet": snippet}

# --- Safe calculator tool ---
def safe_eval_expression(expr: str) -> float:
    """
    Evaluate math expressions safely using math.* namespace only.
    This is intentionally restrictive.
    """
    allowed_names = {k: getattr(math, k) for k in dir(math) if not k.startswith("_")}
    # add builtins we want:
    allowed_names.update({"abs": abs, "round": round, "min": min, "max": max})
    try:
        code = compile(expr, "<expr>", "eval")
        for name in code.co_names:
            if name not in allowed_names:
                raise NameError(f"Use of '{name}' not allowed in expressions")
        result = eval(code, {"__builtins__": {}}, allowed_names)
        return float(result)
    except Exception as e:
        logger.exception("safe_eval_expression failed")
        raise

# --- Mock search tool (for demo) ---
# Generated by Copilot
# filepath: d:\AI-Training\Assignment\Research-Agent\tools.py
async def mock_search(query: str) -> List[Dict[str, str]]:
    """
    Search Wikipedia for real information.
    Returns Wikipedia search results with actual content.
    """
    logger.debug("Searching Wikipedia for query=%s", query)
    
    # Add user agent to avoid 403 errors
    headers = {
        'User-Agent': 'ResearchAgent/1.0 (Educational Project; Python/aiohttp)',
        'Accept': 'application/json'
    }
    
    try:
        # Wikipedia API endpoint
        url = "https://en.wikipedia.org/api/rest_v1/page/summary/" + query.replace(" ", "_")
        
        async with aiohttp.ClientSession(headers=headers) as session:
            async with session.get(url, timeout=10) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    return [
                        {
                            "title": data.get("title", query),
                            "url": data.get("content_urls", {}).get("desktop", {}).get("page", f"https://en.wikipedia.org/wiki/{query}"),
                            "snippet": data.get("extract", "No description available")
                        }
                    ]
                else:
                    # Fallback: Try Wikipedia search API
                    search_url = "https://en.wikipedia.org/w/api.php"
                    params = {
                        "action": "opensearch",
                        "search": query,
                        "limit": 3,
                        "format": "json",
                        "formatversion": "2"
                    }
                    
                    async with session.get(search_url, params=params, timeout=10) as search_resp:
                        if search_resp.status == 200:
                            search_data = await search_resp.json()
                            results = []
                            
                            if len(search_data) >= 4:
                                titles = search_data[1]
                                descriptions = search_data[2]
                                urls = search_data[3]
                                
                                for i in range(min(len(titles), 3)):
                                    results.append({
                                        "title": titles[i],
                                        "url": urls[i],
                                        "snippet": descriptions[i] if i < len(descriptions) else "No description"
                                    })
                            
                            return results if results else [{"title": query, "url": "", "snippet": "No results found"}]
                        else:
                            logger.warning(f"Wikipedia search returned status {search_resp.status}")
                            return [{"title": query, "url": "", "snippet": "No results found"}]
                        
    except Exception as e:
        logger.error(f"Wikipedia search failed: {e}")
        return [{"title": f"Search for {query}", "url": "", "snippet": f"Unable to search Wikipedia at this time. The book 'Can We Be Strangers Again' may not have a Wikipedia article."}]
        